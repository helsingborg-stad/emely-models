*** Use this command to train the UNFINETUNED small blender 90M:

parlai train_model -t SOMETASKS -m transformer/generator --init-model zoo:tutorial_transformer_generator/model --dict-file zoo:tutorial_transformer_generator/model.dict --embedding-size 512 --n-layers 8 --ffn-size 2048 --dropout 0.1 --n-heads 16 --learn-positional-embeddings True --n-positions 512 --variant xlm --activation gelu --fp16 True --text-truncate 512 --label-truncate 128 --dict-tokenizer bpe --dict-lower True -lr 1e-06 --optimizer adamax --lr-scheduler reduceonplateau --gradient-clip 0.1 -veps 0.25 --betas 0.9,0.999 --update-freq 1 --attention-dropout 0.0 --relu-dropout 0.0 --skip-generation True -vp 15 -stim 60 -vme 20000 -bs 16 -vmt ppl -vmm min --save-after-valid True -wblog True --wandb-project EmelyModels --metrics ppl,bleu-4,rouge-L --model-file model-runs/SOMEDIR/model

- obviously replace SOMEDIR and SOMEFILE

Other options:
--dynamic-ing full
--num-epochs 1000
--multitask
--skip-generation False allows using f1,accuracy as metrics 
--wandb-log
--tensorboard-log
-wblog True --wandb-project EmelyModels

To use a plain textfile as dataset(instead of creating a parlai task)
--task fromfile:parlaiformat --fromfile-datapath path/to/file
	--fromfile-datatype-extension true (add this if theres a file_train.txt, file_valid.txt, file_test.txt)




*** Fine tune a real blender 90_M

parlai train_model -t SOMETASKS -m transformer/generator --init-model zoo:blender/blender_90M/model --dict-file zoo:blender/blender_90M/model.dict --embedding-size 512 --n-layers 8 --ffn-size 2048 --dropout 0.1 --n-heads 16 --learn-positional-embeddings True --n-positions 512 --variant xlm --activation gelu --fp16 True --text-truncate 512 --label-truncate 128 --dict-tokenizer bpe --dict-lower True -lr 1e-06 --optimizer adamax --lr-scheduler reduceonplateau --gradient-clip 0.1 -veps 0.25 --betas 0.9,0.999 --update-freq 1 --attention-dropout 0.0 --relu-dropout 0.0 --skip-generation True -vp 15 -stim 60 -vme 20000 -bs 16 -vmt ppl -vmm min --save-after-valid True --metrics ppl,bleu-4,rouge-L -wblog True --wandb-project EmelyModels --model-file model-runs/SOMEDIR/model


tEST 400 NO WANDB
parlai train_model -t internal-nhq,otter --multitask-weights 2,1 -veps 0.25 --attention-dropout 0.0 --batchsize 16 --model transformer/generator --embedding-size 1280 --ffn-size 5120 --variant prelayernorm --n-heads 32 --n-positions 128 --n-encoder-layers 2 --n-decoder-layers 12 --history-add-global-end-token end --delimiter '  ' --dict-tokenizer bytelevelbpe  --dropout 0.1 --fp16 True --init-model zoo:blender/blender_400Mdistill/model --dict-file zoo:blender/blender_400Mdistill/model.dict --label-truncate 128 -lr 7e-06 --lr-scheduler reduceonplateau --lr-scheduler-patience 3 --optimizer mem_eff_adam --relu-dropout 0.0 --activation gelu --save-after-valid True --text-truncate 128 --truncate 128 --warmup_updates 100 --fp16-impl mem_efficient --update-freq 2 --gradient-clip 0.1 --skip-generation True -vp 10 -vmt ppl -vmm min --metrics ppl,bleu-4,rouge-L -wblog True --wandb-project EmelyModels --model-file model-runs/400Mtest/model --num-epochs 100

removed:
--log_every_n_secs 10
--optimzer adam
--batchsize 128

this worked(maybe a fresh dir is what did it?) but validation loss/ppl went really bad:
parlai train_model -t internal-nhq,otter --multitask-weights 2,1 -veps 0.25 --attention-dropout 0.0 --batchsize 16 --model transformer/generator --embedding-size 1280 --ffn-size 5120 --variant prelayernorm --n-heads 32 --n-positions 128 --n-encoder-layers 2 --n-decoder-layers 12 --history-add-global-end-token end --delimiter '  ' --dict-tokenizer bytelevelbpe  --dropout 0.1 --fp16 True --init-model zoo:blender/blender_400Mdistill/model --dict-file zoo:blender/blender_400Mdistill/model.dict --label-truncate 128 -lr 7e-06 --lr-scheduler reduceonplateau --lr-scheduler-patience 3 --optimizer mem_eff_adam --relu-dropout 0.0 --activation gelu --save-after-valid True --text-truncate 128 --truncate 128 --warmup_updates 100 --fp16-impl mem_efficient --update-freq 2 --gradient-clip 0.1 --skip-generation True -vp 10 -vmt ppl -vmm min --metrics ppl,bleu-4,rouge-L -wblog True --wandb-project EmelyModels --model-file model-runs/400Mtest/model --num-epochs 100


*** display_model - Model predictions on dataset
parlai display_model --task TASK -mf model-runs/SOMEDIR/model --datatype test

*** Interactive - chat with model
parlai interactive -mf 


