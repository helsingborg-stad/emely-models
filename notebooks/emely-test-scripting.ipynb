{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88cc90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.agents import create_agent\n",
    "from parlai.agents.emely.emely import EmelyAgent\n",
    "from parlai.core.opt import Opt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from parlai.scripts.torchscript import export_emely\n",
    "from parlai.core.agents import create_agent\n",
    "\n",
    "# Initialize model settings\n",
    "\n",
    "model_path = Path.cwd().parent / 'models/model-runs/major-wood'\n",
    "assert model_path.is_dir()\n",
    "\n",
    "opt_path = model_path / 'model.opt'\n",
    "opt = Opt.load(opt_path)\n",
    "\n",
    "# Change opts\n",
    "opt['skip_generation'] = False\n",
    "opt['init_model'] = (model_path / 'model').as_posix()\n",
    "opt['no_cuda'] = True  # Cloud run doesn't offer gpu support\n",
    "\n",
    "# Inference options\n",
    "opt['inference'] = 'beam' # 'beam' 'greedy'\n",
    "opt['beam_size'] = 10\n",
    "\n",
    "opt[\"scripted_model_file\"] = \"../saved_models/emely_scripted_test.pt\"\n",
    "opt[\"script-module\"] = \"parlai.torchscript.modules:TorchScriptGreedySearch\"\n",
    "opt[\"model_file\"] = opt[\"init_model\"]\n",
    "opt[\"temp_separator\"] = \"__space__\"\n",
    "opt[\"bpe_add_prefix_space\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d061af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:05:56 | loading dictionary from /home/ckjellson/code/emely-models/models/model-runs/major-wood/model.dict\n",
      "10:05:56 | num words = 54944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckjellson/anaconda3/envs/emelymodels/lib/python3.8/site-packages/torch/jit/annotations.py:286: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "  warnings.warn(\"TorchScript will treat type annotations of Tensor \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:05:56 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "10:05:57 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "10:05:57 | Loading existing model params from /home/ckjellson/code/emely-models/models/model-runs/major-wood/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/emely/emely.py:29: UserWarning: Tried to add persona to agent history but EmelyAgent.persona is None\n",
      "  warnings.warn('Tried to add persona to agent history but EmelyAgent.persona is None')\n",
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/transformer/modules/encoder.py:183: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if positions.max().item() > self.n_positions:\n",
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/transformer/modules/attention.py:147: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n",
      "/home/ckjellson/anaconda3/envs/emelymodels/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/transformer/modules/attention.py:153: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale = math.sqrt(dim_per_head)\n",
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/transformer/modules/attention.py:235: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert attn_mask.shape == dot_prod.shape\n",
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/transformer/modules/decoder.py:145: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if positions.max().item() > self.n_positions:\n",
      "/home/ckjellson/anaconda3/envs/emelymodels/lib/python3.8/site-packages/torch/jit/_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../saved_models/emely_scripted_test.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6a9acaeb333f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test exporting to torchscript. This function exports and saves a model at the specified location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquantize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moriginal_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscripted_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_emely\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/emely-models/ParlAI/parlai/scripts/torchscript.py\u001b[0m in \u001b[0;36mexport_emely\u001b[0;34m(opt, quantize)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Script the module and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mscripted_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorchScriptedEmelyAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scripted_model_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscripted_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emelymodels/lib/python3.8/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, buffering, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \"\"\"\n\u001b[1;32m   1011\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_path_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0mkvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_open_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emelymodels/lib/python3.8/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, buffering, encoding, errors, newline, closefd, opener, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[1;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return open(  # type: ignore\n\u001b[0m\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_path_with_cwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../saved_models/emely_scripted_test.pt'"
     ]
    }
   ],
   "source": [
    "# Test exporting to torchscript. This function exports and saves a model at the specified location\n",
    "quantize = False\n",
    "original_module, scripted_module = export_emely(opt,quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0eb8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:26:45 | loading dictionary from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "14:26:45 | num words = 54944\n",
      "14:26:46 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "14:26:46 | Loading existing model params from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n"
     ]
    }
   ],
   "source": [
    "# Create model to test tokenization etc\n",
    "emely_agent = EmelyAgent(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d383cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9797\n",
      "do you have any hobbies , i ' d like to get to know you a bit .\n",
      "do you have any hobbies , i like to write code , coffe , how about you ?\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the output is the same\n",
    "text = \"Hi Emely, how are you?\\nI'm good thanks! What do you do for work?\\nI write code and I drink coffe.\\n\"\n",
    "\n",
    "reply = emely_agent.observe_and_act(text)\n",
    "print(reply)\n",
    "reply = scripted_module(text)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e258a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[792, 813, 3459, 6, 102, 46, 15, 20]\n",
      "[792, 813, 3459, 6, 102, 46, 15, 20]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi Emely, how are you?\"\n",
    "print(emely_agent.dict.txt2vec(text))\n",
    "print(scripted_module.dict.txt2vec(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d2d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that ' s cool . what is your name ? i ' ve never met anyone with the same name .\n",
      "Time using original emely_agent:  0.8299565315246582\n",
      "that ' s cool . what is your name ? i ' ve never met anyone with the same name as me .\n",
      "Time using original module:  0.7110500335693359\n",
      "that ' s cool . what is your name ? i ' ve never met anyone with the same name as me .\n",
      "Time using scripted model:  0.7660255432128906\n"
     ]
    }
   ],
   "source": [
    "# Test performance\n",
    "from time import time\n",
    "nruns = 1\n",
    "t1 = time()\n",
    "for i in range(nruns):\n",
    "    reply = emely_agent.observe_and_act(\"Hi!\\n My name is something and my friend's name is something likewise.\")\n",
    "    print(reply)\n",
    "t2 = time()\n",
    "orig_time = t2-t1\n",
    "print(\"Time using original emely_agent: \", orig_time)\n",
    "for i in range(nruns):\n",
    "    reply = original_module(\"Hi!\\n My name is something and my friend's name is something likewise.\")\n",
    "    print(reply)\n",
    "t3 = time()\n",
    "orig2_time = t3-t2\n",
    "print(\"Time using original module: \", orig2_time)\n",
    "for i in range(nruns):\n",
    "    reply = scripted_module(\"Hi!\\n My name is something and my friend's name is something likewise.\")\n",
    "    print(reply)\n",
    "t4 = time()\n",
    "script_time = t4-t3\n",
    "print(\"Time using scripted model: \", script_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4988dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.message import Message\n",
    "dummy_message = Message()\n",
    "dummy_message['id'] = 'localHuman'\n",
    "dummy_message['text'] = 'Hi'\n",
    "dummy_message['episode_done'] = False\n",
    "dummy_message['label_candidates'] = None\n",
    "#scripted_module.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f14d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'parlai.agents.transformer.modules.encoder.TransformerEncoder'>\n"
     ]
    }
   ],
   "source": [
    "print(type(emely_agent.model.encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4512099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'em@@', 'ely', ',', 'how', 'are', 'you', '?']\n",
      "['hi', 'em@@', 'ely', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "print(emely_agent.dict.tokenize(\"Hi Emely, how are you?\"))\n",
    "print(scripted_module.dict.tokenize(\"Hi Emely, how are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f45ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a3824349d7a2388adf482c7ad3df0b4ac9f78abdc68688ae6ecfc386bc33d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('emelymodels': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
