{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Emely\n",
    "\n",
    "## Run this noteboook in Jupyter to work with WandB\n",
    "\n",
    "This notebook is for training Emely with different configurations.\n",
    "Use the blender_opts dictionary for the standard options.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "The default options for training are located in settings/default_blender_opts.json and settings/run_blender_opts.json. The default_blender_opts are assumed to stay unchanged, while the run_blender_opts can be altered for each model instance.\n",
    "\n",
    "The current options that can be varied between models with default settings are:\n",
    "\n",
    "- init_model: \"zoo:blender/blender_90M/model\",\n",
    "- dict_file: \"zoo:blender/blender_90M/model.dict\",\n",
    "- bs: 16,\n",
    "- betas: \"0.9,0.999\",\n",
    "- lr: 1e-06,\n",
    "- dropout: 0.1,\n",
    "- inference: \"beam\",\n",
    "- beam_size: 10,\n",
    "- beam_min_length: 10,\n",
    "- beam_block_ngram: 3,\n",
    "- wandb_project: \"emely-v0.X\",\n",
    "- task: \"internal,external,external-gpt3\",\n",
    "- multitask_weights: \"6,3,3\",\n",
    "- mutators: null\n",
    "\n",
    "# Steps\n",
    "\n",
    "## 1. Preparation\n",
    "\n",
    "- Log in to WandB using  \"wandb login\" or \"wandb login --relogin\"\n",
    "- Prepare datasets for use by model training\n",
    "- Make sure docker does not need sudo privilegies and is pruned from previous runs to avoid name collisions\n",
    "- Create a copy of this notebook, and name it accordingly (e.g. pipeline-v04.ipynb)\n",
    "\n",
    "## 2. Main options\n",
    "\n",
    "Set the main options. Currently the only supported persona is interview.\n",
    "\n",
    "## 3. Model specific preparations\n",
    "\n",
    "- Run the first three cells of code, and edit the model specific options in temp_opts/model_i_opts.json\n",
    "- Make sure torch.cuda.is_available()=True\n",
    "- Create the same number of new cells as the number of models and paste \"model_names.append(run_training(i))\" for model i in each cell.\n",
    "\n",
    "## 4. Run training\n",
    "\n",
    "Once the above steps are completed, training is run by \"run all cells below\". This will automatically generate model names, upload training logs to wandb, create docker images and dockerfiles\n",
    "\n",
    "## OBS. Testing not yet implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 1\n",
    "wandb_project_name = \"emely-vXX\"\n",
    "persona = \"interview\"\n",
    "err = os.system(\"mkdir ../../models/emely-runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "from subprocess import Popen\n",
    "import torch\n",
    "import names\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model specific settings\n",
    "\n",
    "### Run the following cell, and then edit the model specific settings in the temp_opts json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_opts/run_blender_opts.json\",\"r\") as file:\n",
    "    run_blender_opts = json.load(file)\n",
    "run_blender_opts[\"wandb_project\"] = wandb_project_name\n",
    "for i in range(n_models):\n",
    "    with open(\"temp_opts/model_\" + str(i+1) + \"_opts.json\",\"w\") as file:\n",
    "        json.dump(run_blender_opts, file, sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training function\n",
    "\n",
    "For clarity, the training function is also defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model_id):\n",
    "\n",
    "    with open(\"temp_opts/default_blender_opts.json\",\"r\") as file:\n",
    "        default_blender_opts = json.load(file)\n",
    "\n",
    "    with open(\"temp_opts/model_\" + str(model_id) + \"_opts.json\",\"r\") as file:\n",
    "        run_blender_opts = json.load(file)\n",
    "\n",
    "    # Set name for file and model run on wandb\n",
    "    name = names.get_full_name().replace(\" \", \"_\").lower()\n",
    "    mf = Path.cwd().parents[1].joinpath(f'models/emely-runs/{name}/model')\n",
    "    \n",
    "    # Finalize training opts\n",
    "    run_blender_opts[\"model_file\"] = mf.as_posix()\n",
    "    run_blender_opts[\"wandb_name\"] = name\n",
    "    run_blender_opts.update(default_blender_opts)\n",
    "\n",
    "    # Uncomment the following line to run for one epoch during testing\n",
    "    #run_blender_opts[\"eps\"] = 1\n",
    "\n",
    "    if run_blender_opts[\"mutators\"] is None:\n",
    "        del run_blender_opts[\"mutators\"]\n",
    "    \n",
    "    # Run training\n",
    "    TrainModel.main(**run_blender_opts)\n",
    "\n",
    "    # Wrap up\n",
    "    os.system(f\"parlai vacuum -mf ../../models/emely-runs/{name}/model\")\n",
    "    with open(f\"../../models/emely-runs/{name}/run_opts.json\",\"w\") as file:\n",
    "        json.dump(run_blender_opts, file, sort_keys=False, indent=4)\n",
    "\n",
    "    return name\n",
    "\n",
    "model_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training in separate cells\n",
    "\n",
    "- Create as many cells as models to train and paste \"model_names.append(run_training(i))\", i=1...N, in each cell.\n",
    "- Only the models that are successfully generated will be appended to model_names, and used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:49:08 | building dictionary first...\n",
      "14:49:08 | No model with opt yet at: /home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model(.opt)\n",
      "14:49:08 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /home/ckjellson/code/emely-models/ParlAI/data,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: True,wandb_name: maria_ecton,wandb_project: emely-vXX,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,interactive_mode: False,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None\u001b[0m\n",
      "14:49:08 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --evaltask None --num-epochs -1 --metrics default --tensorboard-log False --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --fp16-impl apex --learningrate 7.5e-06 --max-lr-steps -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "14:49:08 | loading dictionary from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "14:49:08 | num words = 54944\n",
      "14:49:09 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "14:49:09 | Loading existing model params from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n",
      "14:49:09 | Opt:\n",
      "14:49:09 |     activation: gelu\n",
      "14:49:09 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "14:49:09 |     adam_eps: 1e-08\n",
      "14:49:09 |     add_p1_after_newln: False\n",
      "14:49:09 |     aggregate_micro: False\n",
      "14:49:09 |     allow_missing_init_opts: False\n",
      "14:49:09 |     attention_dropout: 0.0\n",
      "14:49:09 |     batchsize: 16\n",
      "14:49:09 |     beam_block_full_context: True\n",
      "14:49:09 |     beam_block_list_filename: None\n",
      "14:49:09 |     beam_block_ngram: 3\n",
      "14:49:09 |     beam_context_block_ngram: -1\n",
      "14:49:09 |     beam_delay: 30\n",
      "14:49:09 |     beam_length_penalty: 0.65\n",
      "14:49:09 |     beam_min_length: 10\n",
      "14:49:09 |     beam_size: 1\n",
      "14:49:09 |     betas: '(0.9, 0.999)'\n",
      "14:49:09 |     bpe_add_prefix_space: None\n",
      "14:49:09 |     bpe_debug: False\n",
      "14:49:09 |     bpe_dropout: None\n",
      "14:49:09 |     bpe_merge: None\n",
      "14:49:09 |     bpe_vocab: None\n",
      "14:49:09 |     compute_tokenized_bleu: False\n",
      "14:49:09 |     datapath: /home/ckjellson/code/emely-models/ParlAI/data\n",
      "14:49:09 |     datatype: train\n",
      "14:49:09 |     delimiter: '\\n'\n",
      "14:49:09 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "14:49:09 |     dict_endtoken: __end__\n",
      "14:49:09 |     dict_file: /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "14:49:09 |     dict_include_test: False\n",
      "14:49:09 |     dict_include_valid: False\n",
      "14:49:09 |     dict_initpath: None\n",
      "14:49:09 |     dict_language: english\n",
      "14:49:09 |     dict_loaded: True\n",
      "14:49:09 |     dict_lower: True\n",
      "14:49:09 |     dict_max_ngram_size: -1\n",
      "14:49:09 |     dict_maxexs: -1\n",
      "14:49:09 |     dict_maxtokens: -1\n",
      "14:49:09 |     dict_minfreq: 0\n",
      "14:49:09 |     dict_nulltoken: __null__\n",
      "14:49:09 |     dict_starttoken: __start__\n",
      "14:49:09 |     dict_textfields: text,labels\n",
      "14:49:09 |     dict_tokenizer: bpe\n",
      "14:49:09 |     dict_unktoken: __unk__\n",
      "14:49:09 |     display_examples: False\n",
      "14:49:09 |     download_path: None\n",
      "14:49:09 |     dropout: 0.1\n",
      "14:49:09 |     dynamic_batching: None\n",
      "14:49:09 |     embedding_projection: random\n",
      "14:49:09 |     embedding_size: 512\n",
      "14:49:09 |     embedding_type: random\n",
      "14:49:09 |     embeddings_scale: True\n",
      "14:49:09 |     eval_batchsize: None\n",
      "14:49:09 |     eval_dynamic_batching: None\n",
      "14:49:09 |     evaltask: internal,external\n",
      "14:49:09 |     ffn_size: 2048\n",
      "14:49:09 |     force_fp16_tokens: False\n",
      "14:49:09 |     fp16: True\n",
      "14:49:09 |     fp16_impl: safe\n",
      "14:49:09 |     gpu: -1\n",
      "14:49:09 |     gradient_clip: 0.1\n",
      "14:49:09 |     hide_labels: False\n",
      "14:49:09 |     history_add_global_end_token: None\n",
      "14:49:09 |     history_reversed: False\n",
      "14:49:09 |     history_size: -1\n",
      "14:49:09 |     image_cropsize: 224\n",
      "14:49:09 |     image_mode: raw\n",
      "14:49:09 |     image_size: 256\n",
      "14:49:09 |     inference: beam\n",
      "14:49:09 |     init_model: /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n",
      "14:49:09 |     init_opt: None\n",
      "14:49:09 |     interactive_mode: False\n",
      "14:49:09 |     invsqrt_lr_decay_gamma: -1\n",
      "14:49:09 |     is_debug: False\n",
      "14:49:09 |     label_truncate: 128\n",
      "14:49:09 |     learn_positional_embeddings: True\n",
      "14:49:09 |     learningrate: 1e-06\n",
      "14:49:09 |     load_from_checkpoint: True\n",
      "14:49:09 |     log_every_n_secs: -1\n",
      "14:49:09 |     log_every_n_steps: 50\n",
      "14:49:09 |     loglevel: info\n",
      "14:49:09 |     lr_scheduler: reduceonplateau\n",
      "14:49:09 |     lr_scheduler_decay: 0.5\n",
      "14:49:09 |     lr_scheduler_patience: 3\n",
      "14:49:10 |     max_train_steps: -1\n",
      "14:49:10 |     max_train_time: -1\n",
      "14:49:10 |     metrics: ppl,bleu-4,rouge-L\n",
      "14:49:10 |     model: transformer/generator\n",
      "14:49:10 |     model_file: /home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model\n",
      "14:49:10 |     model_parallel: False\n",
      "14:49:10 |     momentum: 0\n",
      "14:49:10 |     multitask_weights: (1.0,)\n",
      "14:49:10 |     mutators: None\n",
      "14:49:10 |     n_decoder_layers: -1\n",
      "14:49:10 |     n_encoder_layers: -1\n",
      "14:49:10 |     n_heads: 16\n",
      "14:49:10 |     n_layers: 8\n",
      "14:49:10 |     n_positions: 512\n",
      "14:49:10 |     n_segments: 0\n",
      "14:49:10 |     nesterov: True\n",
      "14:49:10 |     no_cuda: False\n",
      "14:49:10 |     num_epochs: 1.0\n",
      "14:49:10 |     num_workers: 0\n",
      "14:49:10 |     nus: (0.7,)\n",
      "14:49:10 |     optimizer: adamax\n",
      "14:49:10 |     output_scaling: 1.0\n",
      "14:49:10 |     override: \"{'init_model': 'zoo:blender/blender_90M/model', 'dict_file': '/home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict', 'batchsize': 16, 'betas': (0.9, 0.999), 'learningrate': 1e-06, 'dropout': 0.1, 'inference': 'beam', 'beam_size': 1, 'beam_min_length': 10, 'beam_block_ngram': 3, 'wandb_project': 'emely-vXX', 'task': 'minimal', 'multitask_weights': (1.0,), 'model_file': '/home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model', 'wandb_name': 'maria_ecton', 'activation': 'gelu', 'attention_dropout': 0.0, 'dict_lower': True, 'dict_tokenizer': 'bpe', 'embedding_size': 512, 'evaltask': 'internal,external', 'ffn_size': 2048, 'fp16': True, 'gradient_clip': 0.1, 'label_truncate': 128, 'learn_positional_embeddings': True, 'lr_scheduler': 'reduceonplateau', 'metrics': 'ppl,bleu-4,rouge-L', 'model': 'transformer/generator', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'optimizer': 'adamax', 'relu_dropout': 0.0, 'save_after_valid': True, 'skip_generation': False, 'save_every_n_secs': 60.0, 'tensorboard_log': True, 'text_truncate': 512, 'update_freq': 1, 'variant': 'xlm', 'validation_every_n_epochs': 0.25, 'validation_max_exs': 20000, 'validation_metric_mode': 'min', 'validation_metric': 'ppl', 'validation_patience': 15, 'wandb_log': True, 'num_epochs': 1.0}\"\n",
      "14:49:10 |     parlai_home: /home/ckjellson/code/emely-models/ParlAI\n",
      "14:49:10 |     person_tokens: False\n",
      "14:49:10 |     rank_candidates: False\n",
      "14:49:10 |     relu_dropout: 0.0\n",
      "14:49:10 |     save_after_valid: True\n",
      "14:49:10 |     save_every_n_secs: 60.0\n",
      "14:49:10 |     share_word_embeddings: True\n",
      "14:49:10 |     short_final_eval: False\n",
      "14:49:10 |     skip_generation: False\n",
      "14:49:10 |     special_tok_lst: None\n",
      "14:49:10 |     split_lines: False\n",
      "14:49:10 |     starttime: Oct14_14-49\n",
      "14:49:10 |     task: minimal\n",
      "14:49:10 |     temperature: 1.0\n",
      "14:49:10 |     tensorboard_log: True\n",
      "14:49:10 |     tensorboard_logdir: None\n",
      "14:49:10 |     text_truncate: 512\n",
      "14:49:10 |     topk: 10\n",
      "14:49:10 |     topp: 0.9\n",
      "14:49:10 |     truncate: -1\n",
      "14:49:10 |     update_freq: 1\n",
      "14:49:10 |     use_reply: label\n",
      "14:49:10 |     validation_cutoff: 1.0\n",
      "14:49:10 |     validation_every_n_epochs: 0.25\n",
      "14:49:10 |     validation_every_n_secs: -1\n",
      "14:49:10 |     validation_every_n_steps: -1\n",
      "14:49:10 |     validation_max_exs: 20000\n",
      "14:49:10 |     validation_metric: ppl\n",
      "14:49:10 |     validation_metric_mode: min\n",
      "14:49:10 |     validation_patience: 15\n",
      "14:49:10 |     validation_share_agent: False\n",
      "14:49:10 |     variant: xlm\n",
      "14:49:10 |     verbose: False\n",
      "14:49:10 |     wandb_entity: None\n",
      "14:49:10 |     wandb_log: True\n",
      "14:49:10 |     wandb_name: maria_ecton\n",
      "14:49:10 |     wandb_project: emely-vXX\n",
      "14:49:10 |     warmup_rate: 0.0001\n",
      "14:49:10 |     warmup_updates: -1\n",
      "14:49:10 |     weight_decay: None\n",
      "14:49:10 | Current ParlAI commit: fe504c1e1611d22e7dc087026fef78addf57dd77\n",
      "14:49:10 | Current internal commit: fe504c1e1611d22e7dc087026fef78addf57dd77\n",
      "14:49:10 | Current fb commit: fe504c1e1611d22e7dc087026fef78addf57dd77\n",
      "14:49:10 | creating task(s): minimal\n",
      "14:49:10 | Loading ParlAI text data: /home/ckjellson/code/emely-models/ParlAI/data/minimal/train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">maria_ecton</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ckjellson/emely-vXX\" target=\"_blank\">https://wandb.ai/ckjellson/emely-vXX</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ckjellson/emely-vXX/runs/18h1r4j9\" target=\"_blank\">https://wandb.ai/ckjellson/emely-vXX/runs/18h1r4j9</a><br/>\n",
       "                Run data is saved locally in <code>/home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/wandb/run-20211014_144910-18h1r4j9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:49:14 | training...\n",
      "14:49:15 | time:5s total_exs:16 total_steps:1 epochs:16.00 time_left:0s\n",
      "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  llen  loss    lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
      "      41     1   656 481.7       0          0 11.75   16  47.34    11 2.819 1e-06   176 129.2       0          0 16.75   \n",
      "    token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
      "        .3580         0                    1  832 610.9 .7351\n",
      "\n",
      "14:49:15 | num_epochs completed:1.0 time elapsed:5.249457836151123s\n",
      "14:49:15 | Saving dictionary to /home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model.dict\n",
      "14:49:17 | \u001b[33mOverriding opt[\"init_model\"] to zoo:blender/blender_90M/model (previously: /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model)\u001b[0m\n",
      "14:49:17 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n",
      "14:49:17 | \u001b[33mOverriding opt[\"multitask_weights\"] to (1.0,) (previously: [1.0])\u001b[0m\n",
      "14:49:17 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,tensorboard_logdir: None,wandb_log: True,wandb_name: maria_ecton,wandb_project: emely-vXX,wandb_entity: None,mutators: None,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,dict_loaded: True,download_path: None,verbose: False,datapath: /home/ckjellson/code/emely-models/ParlAI/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
      "14:49:17 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--show-advanced-args False --task internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues --numthreads 1 --multitask-weights 1.0,3.0,3.0,3.0 --evaltask None --num-epochs -1 --metrics default --tensorboard-log False --log-every-n-secs 2 --label-type response --include-knowledge True --include-checked-sentence True --include-knowledge-separator False --num-topics 5 --train-experiencer-only False --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --fp16-impl apex --force-fp16-tokens False --learningrate 7.5e-06 --max-lr-steps -1 --parlai-home /private/home/edinan/ParlAI\u001b[0m\n",
      "14:49:17 | loading dictionary from /home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model.dict\n",
      "14:49:17 | num words = 54944\n",
      "14:49:17 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "14:49:17 | Loading existing model params from /home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/model\n",
      "14:49:18 | creating task(s): internal\n",
      "14:49:18 | Loading ParlAI text data: /home/ckjellson/code/emely-models/ParlAI/data/internal/valid.txt\n",
      "14:49:18 | creating task(s): external\n",
      "14:49:18 | Loading ParlAI text data: /home/ckjellson/code/emely-models/ParlAI/data/external/valid.txt\n",
      "14:49:18 | running eval: valid\n",
      "14:49:47 | eval completed in 29.21s\n",
      "14:49:47 | \u001b[1mvalid:\n",
      "             accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  llen  loss    lr  ltpb  ltps  ltrunc  \\\n",
      "   all              0 1.447e-06 53.69 635.2 286.7       0          0 6.369  171 .1334 14.68 2.825 1e-06   190 85.76       0   \n",
      "   external         0 1.846e-06 62.36                   0          0         44 .1291 15.89 3.094                         0   \n",
      "   internal         0 1.049e-06 45.02                   0          0        127 .1376 13.46 2.556                         0   \n",
      "             ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb   tps  \n",
      "   all               0 17.47      .4256         0                    1 825.2 372.5  \n",
      "   external          0 22.06      .3991         0                                   \n",
      "   internal          0 12.88      .4520         0\n",
      "\u001b[0m\n",
      "14:49:47 | creating task(s): internal\n",
      "14:49:47 | Loading ParlAI text data: /home/ckjellson/code/emely-models/ParlAI/data/internal/test.txt\n",
      "14:49:47 | creating task(s): external\n",
      "14:49:47 | Loading ParlAI text data: /home/ckjellson/code/emely-models/ParlAI/data/external/test.txt\n",
      "14:49:47 | running eval: test\n",
      "14:50:17 | eval completed in 30.15s\n",
      "14:50:17 | \u001b[1mtest:\n",
      "             accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  llen  loss    lr  ltpb  ltps  ltrunc  \\\n",
      "   all              0 1.447e-06 53.69 635.2 280.6       0          0 6.234  171 .1334 14.68 2.825 1e-06   190 83.94       0   \n",
      "   external         0 1.846e-06 62.36                   0          0         44 .1291 15.89 3.094                         0   \n",
      "   internal         0 1.049e-06 45.02                   0          0        127 .1376 13.46 2.556                         0   \n",
      "             ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb   tps  \n",
      "   all               0 17.47      .4256         0                    1 825.2 364.6  \n",
      "   external          0 22.06      .3991         0                                   \n",
      "   internal          0 12.88      .4520         0\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11455<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7eb8f2aea34c4caeb875e8877eda44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/wandb/run-20211014_144910-18h1r4j9/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ckjellson/code/emely-models/models/emely-runs/maria_ecton/wandb/run-20211014_144910-18h1r4j9/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>exs/train</td><td>16</td></tr><tr><td>clen/train</td><td>41.0</td></tr><tr><td>ctrunc/train</td><td>0.0</td></tr><tr><td>ctrunclen/train</td><td>0.0</td></tr><tr><td>llen/train</td><td>11.0</td></tr><tr><td>ltrunc/train</td><td>0.0</td></tr><tr><td>ltrunclen/train</td><td>0.0</td></tr><tr><td>loss/train</td><td>2.81868</td></tr><tr><td>ppl/train</td><td>16.75474</td></tr><tr><td>token_acc/train</td><td>0.35795</td></tr><tr><td>token_em/train</td><td>0.0</td></tr><tr><td>exps/train</td><td>11.7466</td></tr><tr><td>ltpb/train</td><td>176.0</td></tr><tr><td>ltps/train</td><td>129.22657</td></tr><tr><td>ctpb/train</td><td>656.0</td></tr><tr><td>ctps/train</td><td>481.67432</td></tr><tr><td>tpb/train</td><td>832.0</td></tr><tr><td>tps/train</td><td>610.90456</td></tr><tr><td>ups/train</td><td>0.73513</td></tr><tr><td>gnorm/train</td><td>47.34444</td></tr><tr><td>clip/train</td><td>1.0</td></tr><tr><td>lr/train</td><td>0.0</td></tr><tr><td>total_train_updates/train</td><td>1</td></tr><tr><td>custom_step</td><td>1</td></tr><tr><td>_runtime</td><td>5</td></tr><tr><td>_timestamp</td><td>1634215755</td></tr><tr><td>_step</td><td>0</td></tr><tr><td>internal/exs/valid</td><td>127</td></tr><tr><td>exs/valid</td><td>171</td></tr><tr><td>internal/accuracy/valid</td><td>0.0</td></tr><tr><td>internal/f1/valid</td><td>0.13764</td></tr><tr><td>internal/bleu-4/valid</td><td>0.0</td></tr><tr><td>internal/clen/valid</td><td>45.01575</td></tr><tr><td>internal/ctrunc/valid</td><td>0.0</td></tr><tr><td>internal/ctrunclen/valid</td><td>0.0</td></tr><tr><td>internal/llen/valid</td><td>13.46457</td></tr><tr><td>internal/ltrunc/valid</td><td>0.0</td></tr><tr><td>internal/ltrunclen/valid</td><td>0.0</td></tr><tr><td>internal/loss/valid</td><td>2.55588</td></tr><tr><td>internal/ppl/valid</td><td>12.88264</td></tr><tr><td>internal/token_acc/valid</td><td>0.45205</td></tr><tr><td>internal/token_em/valid</td><td>0.0</td></tr><tr><td>exps/valid</td><td>6.36943</td></tr><tr><td>ltpb/valid</td><td>190.0</td></tr><tr><td>ltps/valid</td><td>85.76179</td></tr><tr><td>ctpb/valid</td><td>635.22222</td></tr><tr><td>ctps/valid</td><td>286.72549</td></tr><tr><td>tpb/valid</td><td>825.22222</td></tr><tr><td>tps/valid</td><td>372.48734</td></tr><tr><td>lr/valid</td><td>0.0</td></tr><tr><td>total_train_updates/valid</td><td>1</td></tr><tr><td>external/exs/valid</td><td>44</td></tr><tr><td>external/accuracy/valid</td><td>0.0</td></tr><tr><td>external/f1/valid</td><td>0.12913</td></tr><tr><td>external/bleu-4/valid</td><td>0.0</td></tr><tr><td>external/clen/valid</td><td>62.36364</td></tr><tr><td>external/ctrunc/valid</td><td>0.0</td></tr><tr><td>external/ctrunclen/valid</td><td>0.0</td></tr><tr><td>external/llen/valid</td><td>15.88636</td></tr><tr><td>external/ltrunc/valid</td><td>0.0</td></tr><tr><td>external/ltrunclen/valid</td><td>0.0</td></tr><tr><td>external/loss/valid</td><td>3.09379</td></tr><tr><td>external/ppl/valid</td><td>22.06055</td></tr><tr><td>external/token_acc/valid</td><td>0.39914</td></tr><tr><td>external/token_em/valid</td><td>0.0</td></tr><tr><td>accuracy/valid</td><td>0.0</td></tr><tr><td>f1/valid</td><td>0.13339</td></tr><tr><td>bleu-4/valid</td><td>0.0</td></tr><tr><td>clen/valid</td><td>53.68969</td></tr><tr><td>ctrunc/valid</td><td>0.0</td></tr><tr><td>ctrunclen/valid</td><td>0.0</td></tr><tr><td>llen/valid</td><td>14.67547</td></tr><tr><td>ltrunc/valid</td><td>0.0</td></tr><tr><td>ltrunclen/valid</td><td>0.0</td></tr><tr><td>loss/valid</td><td>2.82484</td></tr><tr><td>ppl/valid</td><td>17.4716</td></tr><tr><td>token_acc/valid</td><td>0.42559</td></tr><tr><td>token_em/valid</td><td>0.0</td></tr><tr><td>internal/exs/test</td><td>127</td></tr><tr><td>exs/test</td><td>171</td></tr><tr><td>internal/accuracy/test</td><td>0.0</td></tr><tr><td>internal/f1/test</td><td>0.13764</td></tr><tr><td>internal/bleu-4/test</td><td>0.0</td></tr><tr><td>internal/clen/test</td><td>45.01575</td></tr><tr><td>internal/ctrunc/test</td><td>0.0</td></tr><tr><td>internal/ctrunclen/test</td><td>0.0</td></tr><tr><td>internal/llen/test</td><td>13.46457</td></tr><tr><td>internal/ltrunc/test</td><td>0.0</td></tr><tr><td>internal/ltrunclen/test</td><td>0.0</td></tr><tr><td>internal/loss/test</td><td>2.55588</td></tr><tr><td>internal/ppl/test</td><td>12.88264</td></tr><tr><td>internal/token_acc/test</td><td>0.45205</td></tr><tr><td>internal/token_em/test</td><td>0.0</td></tr><tr><td>exps/test</td><td>6.23413</td></tr><tr><td>ltpb/test</td><td>190.0</td></tr><tr><td>ltps/test</td><td>83.94032</td></tr><tr><td>ctpb/test</td><td>635.22222</td></tr><tr><td>ctps/test</td><td>280.63595</td></tr><tr><td>tpb/test</td><td>825.22222</td></tr><tr><td>tps/test</td><td>364.57646</td></tr><tr><td>lr/test</td><td>0.0</td></tr><tr><td>total_train_updates/test</td><td>1</td></tr><tr><td>external/exs/test</td><td>44</td></tr><tr><td>external/accuracy/test</td><td>0.0</td></tr><tr><td>external/f1/test</td><td>0.12913</td></tr><tr><td>external/bleu-4/test</td><td>0.0</td></tr><tr><td>external/clen/test</td><td>62.36364</td></tr><tr><td>external/ctrunc/test</td><td>0.0</td></tr><tr><td>external/ctrunclen/test</td><td>0.0</td></tr><tr><td>external/llen/test</td><td>15.88636</td></tr><tr><td>external/ltrunc/test</td><td>0.0</td></tr><tr><td>external/ltrunclen/test</td><td>0.0</td></tr><tr><td>external/loss/test</td><td>3.09379</td></tr><tr><td>external/ppl/test</td><td>22.06055</td></tr><tr><td>external/token_acc/test</td><td>0.39914</td></tr><tr><td>external/token_em/test</td><td>0.0</td></tr><tr><td>accuracy/test</td><td>0.0</td></tr><tr><td>f1/test</td><td>0.13339</td></tr><tr><td>bleu-4/test</td><td>0.0</td></tr><tr><td>clen/test</td><td>53.68969</td></tr><tr><td>ctrunc/test</td><td>0.0</td></tr><tr><td>ctrunclen/test</td><td>0.0</td></tr><tr><td>llen/test</td><td>14.67547</td></tr><tr><td>ltrunc/test</td><td>0.0</td></tr><tr><td>ltrunclen/test</td><td>0.0</td></tr><tr><td>loss/test</td><td>2.82484</td></tr><tr><td>ppl/test</td><td>17.4716</td></tr><tr><td>token_acc/test</td><td>0.42559</td></tr><tr><td>token_em/test</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>exs/train</td><td>▁</td></tr><tr><td>clen/train</td><td>▁</td></tr><tr><td>ctrunc/train</td><td>▁</td></tr><tr><td>ctrunclen/train</td><td>▁</td></tr><tr><td>llen/train</td><td>▁</td></tr><tr><td>ltrunc/train</td><td>▁</td></tr><tr><td>ltrunclen/train</td><td>▁</td></tr><tr><td>loss/train</td><td>▁</td></tr><tr><td>ppl/train</td><td>▁</td></tr><tr><td>token_acc/train</td><td>▁</td></tr><tr><td>token_em/train</td><td>▁</td></tr><tr><td>exps/train</td><td>▁</td></tr><tr><td>ltpb/train</td><td>▁</td></tr><tr><td>ltps/train</td><td>▁</td></tr><tr><td>ctpb/train</td><td>▁</td></tr><tr><td>ctps/train</td><td>▁</td></tr><tr><td>tpb/train</td><td>▁</td></tr><tr><td>tps/train</td><td>▁</td></tr><tr><td>ups/train</td><td>▁</td></tr><tr><td>gnorm/train</td><td>▁</td></tr><tr><td>clip/train</td><td>▁</td></tr><tr><td>lr/train</td><td>▁</td></tr><tr><td>total_train_updates/train</td><td>▁</td></tr><tr><td>custom_step</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">maria_ecton</strong>: <a href=\"https://wandb.ai/ckjellson/emely-vXX/runs/18h1r4j9\" target=\"_blank\">https://wandb.ai/ckjellson/emely-vXX/runs/18h1r4j9</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names.append(run_training(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models are trained, now create docker images\n",
    "\n",
    "### Start with creating a local parlai image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = os.system(f\"cp ../parlai-docker/Dockerfile ../../Dockerfile\")\n",
    "err = os.system(f\"docker build -t parlai-emely-base-image ../..\")\n",
    "err = os.system(f\"rm ../../Dockerfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = []\n",
    "for name in model_names:\n",
    "    try:\n",
    "        # Store the Dockerfile that is used in the model directory\n",
    "        with open(f\"../{persona}/Dockerfile\",\"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        lines[2] = f\"COPY models/emely-runs/{name} ./models/interview-model\\n\"\n",
    "        dockerfile = \"\".join(lines)\n",
    "        with open(f\"../../models/emely-runs/{name}/Dockerfile\",\"w\") as file:\n",
    "            file.write(dockerfile)\n",
    "        \n",
    "        # Create docker image\n",
    "        err = os.system(f\"cp ../../models/emely-runs/{name}/Dockerfile ../../Dockerfile\")\n",
    "        err = os.system(f\"docker build -t {name} ../..\")\n",
    "        err = os.system(f\"rm ../../Dockerfile\")\n",
    "\n",
    "        image_names.append(name)\n",
    "    except:\n",
    "        print(f\"Error generating docker-image for model {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run testing (not yet working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tested_names = []\n",
    "    image_names = \"blender-minimal-1-model_1\"\n",
    "    #print(os.system(f\"conda activate {testenv}\"))\n",
    "    for name in image_names:\n",
    "        #try:\n",
    "        p1 = Popen(\"/bin/bash\", stdin=subprocess.PIPE, stdout=subprocess.PIPE, encoding='utf8')\n",
    "        p2 = Popen(\"/bin/bash\", stdin=subprocess.PIPE, stdout=subprocess.PIPE, encoding='utf8')\n",
    "\n",
    "        out,err = p1.communicate(f\"docker run --name {name} -p 8080:8080 {name}\")\n",
    "        print(out)\n",
    "        print(err)\n",
    "        time.sleep(5)\n",
    "        out,err = p2.communicate(f\"conda activate {testenv} ; python ../../../emely-testing/main.py\")\n",
    "        print(err)\n",
    "        print(out)\n",
    "        while p2.poll() is not None:\n",
    "            out = p2.stdout\n",
    "            #print(out)\n",
    "        print(out)\n",
    "\n",
    "        p2.kill()\n",
    "        p1.kill()\n",
    "        # p2 = Popen(\"python ../../../emely-testing/main.py\")\n",
    "\n",
    "        # while True:\n",
    "        #     if p2.poll() is None:\n",
    "        #         break\n",
    "\n",
    "        print(os.system(f\"docker stop {name}\"))\n",
    "        print(os.system(f\"docker rm {name}\"))\n",
    "        # tested_names.append(name)\n",
    "        #except:\n",
    "        #    print(f\"Error testing model {name}\")\n",
    "\n",
    "    #docker run --name blender-minimal-1-model_1  -p 8080:8080 blender-minimal-1-model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully trained and dockerized models:\n",
      "maria_ecton\n"
     ]
    }
   ],
   "source": [
    "print(f\"Successfully trained and dockerized models:\")\n",
    "for name in model_names:    # When testing is implemented this should be \"for name in tested_names:\"\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_names)):\n",
    "    os.system(f\"rm temp_opts/model_{str(i+1)}_opts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- End of pipeline ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some utils to change the default files used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    default_blender_opts = {\n",
    "        \"activation\": \"gelu\",\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"dict_lower\": True,\n",
    "        \"dict_tokenizer\": \"bpe\",\n",
    "        \"embedding_size\": 512,\n",
    "        \"evaltask\": \"internal,external\",\n",
    "        \"ffn_size\": 2048,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_clip\": 0.1,\n",
    "        \"label_truncate\": 128,\n",
    "        \"learn_positional_embeddings\": True,\n",
    "        \"lr_scheduler\": \"reduceonplateau\",\n",
    "        \"metrics\": \"ppl,bleu-4,rouge-L\",\n",
    "        \"model\": \"transformer/generator\",\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 8,\n",
    "        \"n_positions\": 512,\n",
    "        \"optimizer\": \"adamax\",\n",
    "        \"relu_dropout\": 0.0,\n",
    "        \"save_after_valid\": True,\n",
    "        \"skip_generation\": False,\n",
    "        \"stim\": 60,\n",
    "        \"tensorboard_log\": True,\n",
    "        \"text_truncate\": 512,\n",
    "        \"update_freq\": 1,\n",
    "        \"variant\": \"xlm\",\n",
    "        \"veps\": 0.25,\n",
    "        \"vme\": 20000,\n",
    "        \"vmm\": \"min\",\n",
    "        \"vmt\": \"ppl\",\n",
    "        \"vp\": 15,\n",
    "        \"wblog\": True\n",
    "    }\n",
    "    run_blender_opts = {\n",
    "        'init_model': 'zoo:blender/blender_90M/model',\n",
    "        'dict_file': 'zoo:blender/blender_90M/model.dict',\n",
    "        'bs': 16,\n",
    "        'betas': '0.9,0.999',\n",
    "        'lr': 1e-06,\n",
    "        'dropout': 0.1,\n",
    "        'inference': 'beam',\n",
    "        'beam_size': 10,\n",
    "        'beam_min_length': 10,\n",
    "        'beam_block_ngram': 3,\n",
    "        'wandb_project': 'parlaiemely',\n",
    "        'task': 'internal,external,external-gpt3',\n",
    "        'multitask_weights': '6,3,3',\n",
    "        'mutators': None\n",
    "    }\n",
    "\n",
    "    with open(\"temp_opts/default_blender_opts.json\",\"w\") as file:\n",
    "        json.dump(default_blender_opts,file, sort_keys=True, indent=4)\n",
    "    with open(\"temp_opts/run_blender_opts.json\",\"w\") as file:\n",
    "        json.dump(run_blender_opts,file, sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a3824349d7a2388adf482c7ad3df0b4ac9f78abdc68688ae6ecfc386bc33d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('emelymodels': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
