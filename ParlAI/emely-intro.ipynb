{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download the basic blenderbot model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!parlai i -mf zoo:blender/blender_90M/model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13:33:04 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
      "13:33:04 | loading dictionary from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "13:33:04 | num words = 54944\n",
      "13:33:04 | TransformerGenerator: full interactive mode on.\n",
      "13:33:05 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "13:33:05 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "13:33:05 | Loading existing model params from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n",
      "13:33:05 | Opt:\n",
      "13:33:05 |     activation: gelu\n",
      "13:33:05 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "13:33:05 |     adam_eps: 1e-08\n",
      "13:33:05 |     add_p1_after_newln: False\n",
      "13:33:05 |     aggregate_micro: False\n",
      "13:33:05 |     allow_missing_init_opts: False\n",
      "13:33:05 |     attention_dropout: 0.0\n",
      "13:33:05 |     batchsize: 16\n",
      "13:33:05 |     beam_block_full_context: False\n",
      "13:33:05 |     beam_block_list_filename: None\n",
      "13:33:05 |     beam_block_ngram: 3\n",
      "13:33:05 |     beam_context_block_ngram: 3\n",
      "13:33:05 |     beam_delay: 30\n",
      "13:33:05 |     beam_length_penalty: 0.65\n",
      "13:33:05 |     beam_min_length: 20\n",
      "13:33:05 |     beam_size: 10\n",
      "13:33:05 |     betas: '[0.9, 0.999]'\n",
      "13:33:05 |     bpe_add_prefix_space: None\n",
      "13:33:05 |     bpe_debug: False\n",
      "13:33:05 |     bpe_dropout: None\n",
      "13:33:05 |     bpe_merge: None\n",
      "13:33:05 |     bpe_vocab: None\n",
      "13:33:05 |     compute_tokenized_bleu: False\n",
      "13:33:05 |     datapath: /home/ckjellson/code/emely-models/ParlAI/data\n",
      "13:33:05 |     datatype: train\n",
      "13:33:05 |     delimiter: '\\n'\n",
      "13:33:05 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "13:33:05 |     dict_endtoken: __end__\n",
      "13:33:05 |     dict_file: /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "13:33:05 |     dict_include_test: False\n",
      "13:33:05 |     dict_include_valid: False\n",
      "13:33:05 |     dict_initpath: None\n",
      "13:33:05 |     dict_language: english\n",
      "13:33:05 |     dict_loaded: True\n",
      "13:33:05 |     dict_lower: True\n",
      "13:33:05 |     dict_max_ngram_size: -1\n",
      "13:33:05 |     dict_maxexs: -1\n",
      "13:33:05 |     dict_maxtokens: -1\n",
      "13:33:05 |     dict_minfreq: 0\n",
      "13:33:05 |     dict_nulltoken: __null__\n",
      "13:33:05 |     dict_starttoken: __start__\n",
      "13:33:05 |     dict_textfields: text,labels\n",
      "13:33:05 |     dict_tokenizer: bpe\n",
      "13:33:05 |     dict_unktoken: __unk__\n",
      "13:33:05 |     display_add_fields: \n",
      "13:33:05 |     display_examples: False\n",
      "13:33:05 |     display_prettify: False\n",
      "13:33:05 |     download_path: None\n",
      "13:33:05 |     dropout: 0.1\n",
      "13:33:05 |     dynamic_batching: None\n",
      "13:33:05 |     embedding_projection: random\n",
      "13:33:05 |     embedding_size: 512\n",
      "13:33:05 |     embedding_type: random\n",
      "13:33:05 |     embeddings_scale: True\n",
      "13:33:05 |     eval_batchsize: None\n",
      "13:33:05 |     evaltask: None\n",
      "13:33:05 |     ffn_size: 2048\n",
      "13:33:05 |     force_fp16_tokens: True\n",
      "13:33:05 |     fp16: True\n",
      "13:33:05 |     fp16_impl: safe\n",
      "13:33:05 |     gpu: -1\n",
      "13:33:05 |     gradient_clip: 0.1\n",
      "13:33:05 |     hide_labels: False\n",
      "13:33:05 |     history_add_global_end_token: None\n",
      "13:33:05 |     history_reversed: False\n",
      "13:33:05 |     history_size: -1\n",
      "13:33:05 |     image_cropsize: 224\n",
      "13:33:05 |     image_mode: raw\n",
      "13:33:05 |     image_size: 256\n",
      "13:33:05 |     include_checked_sentence: True\n",
      "13:33:05 |     include_knowledge: True\n",
      "13:33:05 |     include_knowledge_separator: False\n",
      "13:33:05 |     inference: beam\n",
      "13:33:05 |     init_model: /checkpoint/parlai/zoo/new_reddit/newreddit_trained20190909_usedfordodeca/model\n",
      "13:33:05 |     init_opt: None\n",
      "13:33:05 |     interactive_mode: True\n",
      "13:33:05 |     interactive_task: True\n",
      "13:33:05 |     invsqrt_lr_decay_gamma: -1\n",
      "13:33:05 |     is_debug: False\n",
      "13:33:05 |     label_truncate: 128\n",
      "13:33:05 |     label_type: response\n",
      "13:33:05 |     learn_positional_embeddings: True\n",
      "13:33:05 |     learningrate: 7.5e-06\n",
      "13:33:05 |     local_human_candidates_file: None\n",
      "13:33:05 |     log_every_n_secs: 2\n",
      "13:33:05 |     log_keep_fields: all\n",
      "13:33:05 |     loglevel: info\n",
      "13:33:05 |     lr_scheduler: reduceonplateau\n",
      "13:33:05 |     lr_scheduler_decay: 0.5\n",
      "13:33:05 |     lr_scheduler_patience: 3\n",
      "13:33:05 |     max_lr_steps: -1\n",
      "13:33:05 |     max_train_time: -1\n",
      "13:33:05 |     metrics: default\n",
      "13:33:05 |     model: transformer/generator\n",
      "13:33:05 |     model_file: /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n",
      "13:33:05 |     model_parallel: False\n",
      "13:33:05 |     momentum: 0\n",
      "13:33:05 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
      "13:33:05 |     n_decoder_layers: -1\n",
      "13:33:05 |     n_encoder_layers: -1\n",
      "13:33:05 |     n_heads: 16\n",
      "13:33:05 |     n_layers: 8\n",
      "13:33:05 |     n_positions: 512\n",
      "13:33:05 |     n_segments: 0\n",
      "13:33:05 |     nesterov: True\n",
      "13:33:05 |     no_cuda: False\n",
      "13:33:05 |     num_epochs: -1\n",
      "13:33:05 |     num_topics: 5\n",
      "13:33:05 |     numthreads: 1\n",
      "13:33:05 |     nus: [0.7]\n",
      "13:33:05 |     optimizer: adamax\n",
      "13:33:05 |     outfile: \n",
      "13:33:05 |     output_scaling: 1.0\n",
      "13:33:05 |     override: {}\n",
      "13:33:05 |     parlai_home: /private/home/edinan/ParlAI\n",
      "13:33:05 |     person_tokens: False\n",
      "13:33:05 |     rank_candidates: False\n",
      "13:33:05 |     relu_dropout: 0.0\n",
      "13:33:05 |     save_after_valid: True\n",
      "13:33:05 |     save_every_n_secs: 60.0\n",
      "13:33:05 |     save_format: conversations\n",
      "13:33:05 |     share_word_embeddings: True\n",
      "13:33:05 |     short_final_eval: False\n",
      "13:33:05 |     show_advanced_args: False\n",
      "13:33:05 |     single_turn: False\n",
      "13:33:05 |     skip_generation: False\n",
      "13:33:05 |     special_tok_lst: None\n",
      "13:33:05 |     split_lines: False\n",
      "13:33:05 |     starttime: Feb10_07-25\n",
      "13:33:05 |     task: internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues\n",
      "13:33:05 |     temperature: 1.0\n",
      "13:33:05 |     tensorboard_log: False\n",
      "13:33:05 |     text_truncate: 512\n",
      "13:33:05 |     topk: 10\n",
      "13:33:05 |     topp: 0.9\n",
      "13:33:05 |     train_experiencer_only: False\n",
      "13:33:05 |     truncate: -1\n",
      "13:33:05 |     update_freq: 1\n",
      "13:33:05 |     use_reply: label\n",
      "13:33:05 |     validation_cutoff: 1.0\n",
      "13:33:05 |     validation_every_n_epochs: 0.25\n",
      "13:33:05 |     validation_every_n_secs: -1\n",
      "13:33:05 |     validation_max_exs: 20000\n",
      "13:33:05 |     validation_metric: ppl\n",
      "13:33:05 |     validation_metric_mode: min\n",
      "13:33:05 |     validation_patience: 15\n",
      "13:33:05 |     validation_share_agent: False\n",
      "13:33:05 |     variant: xlm\n",
      "13:33:05 |     verbose: False\n",
      "13:33:05 |     warmup_rate: 0.0001\n",
      "13:33:05 |     warmup_updates: -1\n",
      "13:33:05 |     weight_decay: None\n",
      "13:33:05 | Current ParlAI commit: 09c9adf69c10829108a5b418cf934a581f684700\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "13:33:05 | creating task(s): interactive\n",
      "\u001b[0mEnter Your Message:\u001b[0;0m ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ckjellson/anaconda3/envs/emelymodels/bin/parlai\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('parlai', 'console_scripts', 'parlai')())\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/__main__.py\", line 14, in main\n",
      "    superscript_main()\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/core/script.py\", line 306, in superscript_main\n",
      "    return SCRIPT_REGISTRY[cmd].klass._run_from_parser_and_opt(opt, parser)\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/core/script.py\", line 89, in _run_from_parser_and_opt\n",
      "    return script.run()\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/scripts/interactive.py\", line 118, in run\n",
      "    return interactive(self.opt)\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/scripts/interactive.py\", line 93, in interactive\n",
      "    world.parley()\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/tasks/interactive/worlds.py\", line 75, in parley\n",
      "    act = deepcopy(agents[0].act())\n",
      "  File \"/home/ckjellson/code/emely-models/ParlAI/parlai/agents/local_human/local_human.py\", line 75, in act\n",
      "    reply_text = input(colorize(\"Enter Your Message:\", 'text') + ' ')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from parlai.core.agents import create_agent\n",
    "from parlai.agents.emely.emely import EmelyAgent\n",
    "from parlai.core.opt import Opt\n",
    "from pathlib import Path\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_path = Path.cwd() / 'data/models/blender/blender_90M/'\n",
    "assert model_path.is_dir()\n",
    "\n",
    "opt_path = model_path / 'model.opt'\n",
    "opt = Opt.load(opt_path)\n",
    "\n",
    "# Change opts \n",
    "opt['skip_generation'] = False\n",
    "opt['init_model'] = (model_path / 'model').as_posix()\n",
    "opt['no_cuda'] = True  # Cloud run doesn't offer gpu support\n",
    "\n",
    "# Inference options\n",
    "opt['inference'] = 'greedy' # 'beam'\n",
    "opt['beam_size'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This is how we use Emely currently\n",
    "\n",
    "EmelyAgent subclasses *TransformerGeneratorAgent* and has the extra method observe_and_act(text)\n",
    "\n",
    "EmelyAgent has many attributes (and methods), but importantly:\n",
    "\n",
    "- EmelyAgent.model: the pytorch model of the transformer\n",
    "    - model.encoder: the transformer encoder\n",
    "    - model.decoder: the transformer decoder\n",
    "    \n",
    "    \n",
    "- EmelyAgent.history: the conversation history, which is used when \n",
    "\n",
    "- EmelyAgent.observe(): observes a new message and adds to the history\n",
    "- EmelyAgent.self_observe: observes it's own response and adds to the history\n",
    "\n",
    "- EmelyAgent.act(): creates a new response based on the history. high level function\n",
    "- EmelyAgent.eval_step(): model inference used to get the raw model output\n",
    "- EmelyAgent._generate(): used to generate a response from the raw model output. Calls beam search or topk sampling\n",
    "\n",
    "\n",
    "The parlai *TransformerGeneratorAgent* object has a history, from which the context is built and passed thorugh the model to generate a reply. But due to Emely handling several conversations simultaneously, we send the entire dialog history we want the model to act on (this is usually the last 6-8 messages) and the method observe_and_act() builds the history and then calls act()\n",
    "\n",
    "\n",
    "#### Text format. Messages are separated by \\n  \n",
    "text = 'hey there\\nHi my name is Emely, how are you today?\\ngood thanks. What do you work with?'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "emely_agent = EmelyAgent(opt)\n",
    "# Option to quantize the mdoel in torch to speed up a little bit\n",
    "# emely_agent.model = torch.quantization.quantize_dynamic(emely_agent.model, {torch.nn.Linear}, dtype=torch.qint8) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13:44:28 | loading dictionary from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "13:44:28 | num words = 54944\n",
      "13:44:28 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "13:44:29 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "13:44:29 | Loading existing model params from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/emely/emely.py:29: UserWarning: Tried to add persona to agent history but EmelyAgent.persona is None\n",
      "  warnings.warn('Tried to add persona to agent history but EmelyAgent.persona is None')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Try passing text through emely"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# This is a conversation where the human has written two messages and Emely one. \n",
    "text = \"Hi Emely, how are you?\\nI'm good thanks! What do you do for work?\\nI write code and I drink coffe\"\n",
    "emely_agent.observe_and_act(text)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"that ' s cool ! i ' ve never tried coffee . i ' d love to try it though .\""
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from torchinfo import summary\n",
    "print(summary(emely_agent.model))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "TransformerGeneratorModel                     --\n",
      "├─Embedding: 1-1                              28,131,328\n",
      "├─TransformerEncoder: 1-2                     --\n",
      "│    └─Dropout: 2-1                           --\n",
      "│    └─Embedding: 2-2                         (recursive)\n",
      "│    └─Embedding: 2-3                         262,144\n",
      "│    └─LayerNorm: 2-4                         1,024\n",
      "│    └─ModuleList: 2-5                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-1      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-2      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-3      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-4      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-5      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-6      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-7      3,152,384\n",
      "│    │    └─TransformerEncoderLayer: 3-8      3,152,384\n",
      "├─TransformerDecoder: 1-3                     --\n",
      "│    └─Dropout: 2-6                           --\n",
      "│    └─Embedding: 2-7                         (recursive)\n",
      "│    └─LayerNorm: 2-8                         1,024\n",
      "│    └─Embedding: 2-9                         262,144\n",
      "│    └─ModuleList: 2-10                       --\n",
      "│    │    └─TransformerDecoderLayer: 3-9      4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-10     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-11     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-12     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-13     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-14     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-15     4,204,032\n",
      "│    │    └─TransformerDecoderLayer: 3-16     4,204,032\n",
      "======================================================================\n",
      "Total params: 87,508,992\n",
      "Trainable params: 87,508,992\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PDB debug of observe_and_act()\n",
    "\n",
    "#### the observe_and_act() method\n",
    "1. Build history from text\n",
    "2. act(). During act the history is vectorized and passed through the model. This method is quite deep and is good to debug to get an understanding of"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def debug():\n",
    "    #set_trace()\n",
    "    reply = emely_agent.observe_and_act(text)\n",
    "    reply = emely_agent.observe_and_act(text)\n",
    "    \n",
    "debug()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> \u001b[0;32m<ipython-input-14-cae48e672ceb>\u001b[0m(5)\u001b[0;36mdebug\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      3 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 5 \u001b[0;31m    \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memely_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_and_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m    \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memely_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_and_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ipdb>  5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ipdb>  rgsef\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Notes and scraps below\n",
    "\n",
    "Unstructured notes and comments/code\n",
    "\n",
    "### batch_act(observations)\n",
    "- observations: list(Message)\n",
    "- observation: Message\n",
    "        {'id': 'localHuman', 'text': 'Hi', 'episode_done': False, 'label_candidates': None, 'full_text': 'Hi', 'text_vec':      tensor([792]), 'full_text_vec': [792], 'context_original_length': 1, 'context_truncate_rate': False, 'context_truncated_length': 0}\n",
    "        \n",
    "        \n",
    "batch = self.batchify(observations) -> batch: Batch\n",
    "num_observations = len(observations)\n",
    "\n",
    "eval_step(self, batch)\n",
    "batch: Batch\n",
    "Batch({\n",
    "  _context_original_length: LongTensor[1],\n",
    "  _context_truncate_rate: LongTensor[1],\n",
    "  _context_truncated_length: LongTensor[1],\n",
    "  _label_original_length: None,\n",
    "  _label_truncate_rate: None,\n",
    "  _label_truncated_length: None,\n",
    "  batchsize: 1,\n",
    "  candidate_vecs: None,\n",
    "  candidates: None,\n",
    "  image: None,\n",
    "  is_training: False,\n",
    "  label_lengths: None,\n",
    "  label_vec: None,\n",
    "  labels: None,\n",
    "  observations: None (use --debug to include),\n",
    "  rewards: None,\n",
    "  text_lengths: None,\n",
    "  text_vec: LongTensor[1, 1],\n",
    "  valid_indices: LongTensor[1],\n",
    "})\n",
    "\n",
    "bsz = batch.text_vec.size(0) # Length of text vector\n",
    "self.model.eval()\n",
    "prefix_tokens = self.get_prefix_tokens(batch)\n",
    "beam_preds_scores, beams = self._generate(\n",
    "    batch, self.beam_size, maxlen, prefix_tokens=prefix_tokens\n",
    ")\n",
    "preds, scores = zip(*beam_preds_scores)\n",
    "\n",
    "### _generate()\n",
    "\n",
    "encoder_states = model.encoder(*self._encoder_input(batch)) - ## WHEN/HOW WAS self._encoder_input(batch)  \n",
    "encoder_states[0].shape\n",
    "torch.Size([1, 1, 512])\n",
    "\n",
    "beams = [\n",
    "                self._treesearch_factory(dev)\n",
    "                .set_batch_context(batch_context_list, batch_idx)\n",
    "                .set_block_list(self.beam_block_list)\n",
    "                for batch_idx in range(batchsize)\n",
    "            ] wtf happens here?\n",
    "            \n",
    "#### self._treesearch_factory -> TopKSampling / BeamSearch\n",
    "\n",
    "return TopKSampling(\n",
    "                self.opt['topk'],\n",
    "                beam_size,\n",
    "                min_length=self.beam_min_length,\n",
    "                block_ngram=self.beam_block_ngram,\n",
    "                context_block_ngram=self.beam_context_block_ngram,\n",
    "                length_penalty=self.opt.get('beam_length_penalty', 0.65),\n",
    "                padding_token=self.NULL_IDX,\n",
    "                bos_token=self.START_IDX,\n",
    "                eos_token=self.END_IDX,\n",
    "                device=device,\n",
    "            )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from parlai.core.message import Message\n",
    "from parlai.core.torch_agent import History\n",
    "from parlai.core.worlds import validate\n",
    "\n",
    "sample_message = Message()\n",
    "sample_message['text'] = text\n",
    "sample_message['id'] = 'localHuman' # 'TransformerGenerator'\n",
    "sample_message['episode_done'] = False\n",
    "sample_message['label_candidates'] = None\n",
    "validate(sample_message)\n",
    "\n",
    "emely_agent.observe(sample_message)\n",
    "\n",
    "history = emely_agent.history\n",
    "#emely_agent.vectorize(message, history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dir(history)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_person_tokens',\n",
       " '_global_end_token',\n",
       " '_update_raw_strings',\n",
       " '_update_strings',\n",
       " '_update_vecs',\n",
       " 'add_p1_after_newln',\n",
       " 'add_person_tokens',\n",
       " 'add_reply',\n",
       " 'delimiter',\n",
       " 'delimiter_tok',\n",
       " 'dict',\n",
       " 'field',\n",
       " 'get_history_str',\n",
       " 'get_history_vec',\n",
       " 'get_history_vec_list',\n",
       " 'history_raw_strings',\n",
       " 'history_strings',\n",
       " 'history_vecs',\n",
       " 'max_len',\n",
       " 'p1_token',\n",
       " 'p2_token',\n",
       " 'parse',\n",
       " 'reset',\n",
       " 'reversed',\n",
       " 'size',\n",
       " 'split_on_newln',\n",
       " 'temp_history',\n",
       " 'update_history']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from time import time\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "for i in range(100):\n",
    "    emely_agent.observe_and_act(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t2 = time()\n",
    "elapsed = t2 - t1\n",
    "elapsed"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38.942471981048584"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "57 sek med beam =10\n",
    "39 sek med greedy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "emely_agent.opt['inference'] = 'greedy'\n",
    "emely_agent.opt['beam_size'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('emelymodels': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "f8da2b72f9ac3c32718e899b9529d2abac5ccc10881893810be77cced976bf1a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}