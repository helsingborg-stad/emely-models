{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from parlai.core.agents import create_agent\n",
    "from parlai.agents.emely.emely import EmelyAgent\n",
    "from parlai.core.opt import Opt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from parlai.scripts.torchscript import export_emely\n",
    "from parlai.core.agents import create_agent\n",
    "\n",
    "# Initialize model settings\n",
    "\n",
    "model_path = Path.cwd() / 'data/models/blender/blender_90M/'\n",
    "assert model_path.is_dir()\n",
    "\n",
    "opt_path = model_path / 'model.opt'\n",
    "opt = Opt.load(opt_path)\n",
    "\n",
    "# Change opts\n",
    "opt['skip_generation'] = False\n",
    "opt['init_model'] = (model_path / 'model').as_posix()\n",
    "opt['no_cuda'] = True  # Cloud run doesn't offer gpu support\n",
    "\n",
    "# Inference options\n",
    "opt['inference'] = 'beam' # 'beam' 'greedy'\n",
    "opt['beam_size'] = 10\n",
    "\n",
    "opt[\"scripted_model_file\"] = \"../../saved_models/emely_scripted_test_quantized.pt\"\n",
    "opt[\"script-module\"] = \"parlai.torchscript.modules:TorchScriptGreedySearch\"\n",
    "opt[\"model_file\"] = opt[\"init_model\"]\n",
    "\n",
    "opt[\"temp_separator\"] = \"__space__\"\n",
    "\n",
    "opt[\"bpe_add_prefix_space\"] = False\n",
    "\n",
    "opt[\"input\"] = \"Hi! What do you like to do?\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "emely_agent = EmelyAgent(opt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19:10:07 | loading dictionary from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model.dict\n",
      "19:10:08 | num words = 54944\n",
      "19:10:08 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "19:10:09 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "19:10:09 | Loading existing model params from /home/ckjellson/code/emely-models/ParlAI/data/models/blender/blender_90M/model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ckjellson/code/emely-models/ParlAI/parlai/agents/emely/emely.py:29: UserWarning: Tried to add persona to agent history but EmelyAgent.persona is None\n",
      "  warnings.warn('Tried to add persona to agent history but EmelyAgent.persona is None')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "weights = torch.load(\"../../huggingface/blenderbot_small-90M/pytorch_model.bin\")\n",
    "#print(weights.keys())\n",
    "weights2 = weights.copy()\n",
    "for k,v in weights.items():\n",
    "    if k[:6]==\"model.\":\n",
    "        weights2[k[6:]] = v\n",
    "print(weights2.keys())\n",
    "#emely_agent.model.load_state_dict(weights2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "odict_keys(['final_logits_bias', 'model.shared.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias', 'lm_head.weight', 'shared.weight', 'encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.6.fc2.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layernorm_embedding.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.fc1.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.6.fc2.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.7.fc1.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layernorm_embedding.weight', 'decoder.layernorm_embedding.bias'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(emely_agent.state_dict()['model'].keys())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "odict_keys(['START', 'embeddings.weight', 'encoder.embeddings.weight', 'encoder.position_embeddings.weight', 'encoder.norm_embeddings.weight', 'encoder.norm_embeddings.bias', 'encoder.layers.0.attention.q_lin.weight', 'encoder.layers.0.attention.q_lin.bias', 'encoder.layers.0.attention.k_lin.weight', 'encoder.layers.0.attention.k_lin.bias', 'encoder.layers.0.attention.v_lin.weight', 'encoder.layers.0.attention.v_lin.bias', 'encoder.layers.0.attention.out_lin.weight', 'encoder.layers.0.attention.out_lin.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.ffn.lin1.weight', 'encoder.layers.0.ffn.lin1.bias', 'encoder.layers.0.ffn.lin2.weight', 'encoder.layers.0.ffn.lin2.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.1.attention.q_lin.weight', 'encoder.layers.1.attention.q_lin.bias', 'encoder.layers.1.attention.k_lin.weight', 'encoder.layers.1.attention.k_lin.bias', 'encoder.layers.1.attention.v_lin.weight', 'encoder.layers.1.attention.v_lin.bias', 'encoder.layers.1.attention.out_lin.weight', 'encoder.layers.1.attention.out_lin.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.ffn.lin1.weight', 'encoder.layers.1.ffn.lin1.bias', 'encoder.layers.1.ffn.lin2.weight', 'encoder.layers.1.ffn.lin2.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.2.attention.q_lin.weight', 'encoder.layers.2.attention.q_lin.bias', 'encoder.layers.2.attention.k_lin.weight', 'encoder.layers.2.attention.k_lin.bias', 'encoder.layers.2.attention.v_lin.weight', 'encoder.layers.2.attention.v_lin.bias', 'encoder.layers.2.attention.out_lin.weight', 'encoder.layers.2.attention.out_lin.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.2.ffn.lin1.weight', 'encoder.layers.2.ffn.lin1.bias', 'encoder.layers.2.ffn.lin2.weight', 'encoder.layers.2.ffn.lin2.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.3.attention.q_lin.weight', 'encoder.layers.3.attention.q_lin.bias', 'encoder.layers.3.attention.k_lin.weight', 'encoder.layers.3.attention.k_lin.bias', 'encoder.layers.3.attention.v_lin.weight', 'encoder.layers.3.attention.v_lin.bias', 'encoder.layers.3.attention.out_lin.weight', 'encoder.layers.3.attention.out_lin.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.3.ffn.lin1.weight', 'encoder.layers.3.ffn.lin1.bias', 'encoder.layers.3.ffn.lin2.weight', 'encoder.layers.3.ffn.lin2.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.4.attention.q_lin.weight', 'encoder.layers.4.attention.q_lin.bias', 'encoder.layers.4.attention.k_lin.weight', 'encoder.layers.4.attention.k_lin.bias', 'encoder.layers.4.attention.v_lin.weight', 'encoder.layers.4.attention.v_lin.bias', 'encoder.layers.4.attention.out_lin.weight', 'encoder.layers.4.attention.out_lin.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.4.ffn.lin1.weight', 'encoder.layers.4.ffn.lin1.bias', 'encoder.layers.4.ffn.lin2.weight', 'encoder.layers.4.ffn.lin2.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.5.attention.q_lin.weight', 'encoder.layers.5.attention.q_lin.bias', 'encoder.layers.5.attention.k_lin.weight', 'encoder.layers.5.attention.k_lin.bias', 'encoder.layers.5.attention.v_lin.weight', 'encoder.layers.5.attention.v_lin.bias', 'encoder.layers.5.attention.out_lin.weight', 'encoder.layers.5.attention.out_lin.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.5.ffn.lin1.weight', 'encoder.layers.5.ffn.lin1.bias', 'encoder.layers.5.ffn.lin2.weight', 'encoder.layers.5.ffn.lin2.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.6.attention.q_lin.weight', 'encoder.layers.6.attention.q_lin.bias', 'encoder.layers.6.attention.k_lin.weight', 'encoder.layers.6.attention.k_lin.bias', 'encoder.layers.6.attention.v_lin.weight', 'encoder.layers.6.attention.v_lin.bias', 'encoder.layers.6.attention.out_lin.weight', 'encoder.layers.6.attention.out_lin.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.6.ffn.lin1.weight', 'encoder.layers.6.ffn.lin1.bias', 'encoder.layers.6.ffn.lin2.weight', 'encoder.layers.6.ffn.lin2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.7.attention.q_lin.weight', 'encoder.layers.7.attention.q_lin.bias', 'encoder.layers.7.attention.k_lin.weight', 'encoder.layers.7.attention.k_lin.bias', 'encoder.layers.7.attention.v_lin.weight', 'encoder.layers.7.attention.v_lin.bias', 'encoder.layers.7.attention.out_lin.weight', 'encoder.layers.7.attention.out_lin.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.7.ffn.lin1.weight', 'encoder.layers.7.ffn.lin1.bias', 'encoder.layers.7.ffn.lin2.weight', 'encoder.layers.7.ffn.lin2.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.7.norm2.bias', 'decoder.embeddings.weight', 'decoder.norm_embeddings.weight', 'decoder.norm_embeddings.bias', 'decoder.position_embeddings.weight', 'decoder.layers.0.self_attention.q_lin.weight', 'decoder.layers.0.self_attention.q_lin.bias', 'decoder.layers.0.self_attention.k_lin.weight', 'decoder.layers.0.self_attention.k_lin.bias', 'decoder.layers.0.self_attention.v_lin.weight', 'decoder.layers.0.self_attention.v_lin.bias', 'decoder.layers.0.self_attention.out_lin.weight', 'decoder.layers.0.self_attention.out_lin.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.0.norm1.bias', 'decoder.layers.0.encoder_attention.q_lin.weight', 'decoder.layers.0.encoder_attention.q_lin.bias', 'decoder.layers.0.encoder_attention.k_lin.weight', 'decoder.layers.0.encoder_attention.k_lin.bias', 'decoder.layers.0.encoder_attention.v_lin.weight', 'decoder.layers.0.encoder_attention.v_lin.bias', 'decoder.layers.0.encoder_attention.out_lin.weight', 'decoder.layers.0.encoder_attention.out_lin.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.0.ffn.lin1.weight', 'decoder.layers.0.ffn.lin1.bias', 'decoder.layers.0.ffn.lin2.weight', 'decoder.layers.0.ffn.lin2.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.1.self_attention.q_lin.weight', 'decoder.layers.1.self_attention.q_lin.bias', 'decoder.layers.1.self_attention.k_lin.weight', 'decoder.layers.1.self_attention.k_lin.bias', 'decoder.layers.1.self_attention.v_lin.weight', 'decoder.layers.1.self_attention.v_lin.bias', 'decoder.layers.1.self_attention.out_lin.weight', 'decoder.layers.1.self_attention.out_lin.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.1.norm1.bias', 'decoder.layers.1.encoder_attention.q_lin.weight', 'decoder.layers.1.encoder_attention.q_lin.bias', 'decoder.layers.1.encoder_attention.k_lin.weight', 'decoder.layers.1.encoder_attention.k_lin.bias', 'decoder.layers.1.encoder_attention.v_lin.weight', 'decoder.layers.1.encoder_attention.v_lin.bias', 'decoder.layers.1.encoder_attention.out_lin.weight', 'decoder.layers.1.encoder_attention.out_lin.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.1.ffn.lin1.weight', 'decoder.layers.1.ffn.lin1.bias', 'decoder.layers.1.ffn.lin2.weight', 'decoder.layers.1.ffn.lin2.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.norm3.bias', 'decoder.layers.2.self_attention.q_lin.weight', 'decoder.layers.2.self_attention.q_lin.bias', 'decoder.layers.2.self_attention.k_lin.weight', 'decoder.layers.2.self_attention.k_lin.bias', 'decoder.layers.2.self_attention.v_lin.weight', 'decoder.layers.2.self_attention.v_lin.bias', 'decoder.layers.2.self_attention.out_lin.weight', 'decoder.layers.2.self_attention.out_lin.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.2.norm1.bias', 'decoder.layers.2.encoder_attention.q_lin.weight', 'decoder.layers.2.encoder_attention.q_lin.bias', 'decoder.layers.2.encoder_attention.k_lin.weight', 'decoder.layers.2.encoder_attention.k_lin.bias', 'decoder.layers.2.encoder_attention.v_lin.weight', 'decoder.layers.2.encoder_attention.v_lin.bias', 'decoder.layers.2.encoder_attention.out_lin.weight', 'decoder.layers.2.encoder_attention.out_lin.bias', 'decoder.layers.2.norm2.weight', 'decoder.layers.2.norm2.bias', 'decoder.layers.2.ffn.lin1.weight', 'decoder.layers.2.ffn.lin1.bias', 'decoder.layers.2.ffn.lin2.weight', 'decoder.layers.2.ffn.lin2.bias', 'decoder.layers.2.norm3.weight', 'decoder.layers.2.norm3.bias', 'decoder.layers.3.self_attention.q_lin.weight', 'decoder.layers.3.self_attention.q_lin.bias', 'decoder.layers.3.self_attention.k_lin.weight', 'decoder.layers.3.self_attention.k_lin.bias', 'decoder.layers.3.self_attention.v_lin.weight', 'decoder.layers.3.self_attention.v_lin.bias', 'decoder.layers.3.self_attention.out_lin.weight', 'decoder.layers.3.self_attention.out_lin.bias', 'decoder.layers.3.norm1.weight', 'decoder.layers.3.norm1.bias', 'decoder.layers.3.encoder_attention.q_lin.weight', 'decoder.layers.3.encoder_attention.q_lin.bias', 'decoder.layers.3.encoder_attention.k_lin.weight', 'decoder.layers.3.encoder_attention.k_lin.bias', 'decoder.layers.3.encoder_attention.v_lin.weight', 'decoder.layers.3.encoder_attention.v_lin.bias', 'decoder.layers.3.encoder_attention.out_lin.weight', 'decoder.layers.3.encoder_attention.out_lin.bias', 'decoder.layers.3.norm2.weight', 'decoder.layers.3.norm2.bias', 'decoder.layers.3.ffn.lin1.weight', 'decoder.layers.3.ffn.lin1.bias', 'decoder.layers.3.ffn.lin2.weight', 'decoder.layers.3.ffn.lin2.bias', 'decoder.layers.3.norm3.weight', 'decoder.layers.3.norm3.bias', 'decoder.layers.4.self_attention.q_lin.weight', 'decoder.layers.4.self_attention.q_lin.bias', 'decoder.layers.4.self_attention.k_lin.weight', 'decoder.layers.4.self_attention.k_lin.bias', 'decoder.layers.4.self_attention.v_lin.weight', 'decoder.layers.4.self_attention.v_lin.bias', 'decoder.layers.4.self_attention.out_lin.weight', 'decoder.layers.4.self_attention.out_lin.bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.4.encoder_attention.q_lin.weight', 'decoder.layers.4.encoder_attention.q_lin.bias', 'decoder.layers.4.encoder_attention.k_lin.weight', 'decoder.layers.4.encoder_attention.k_lin.bias', 'decoder.layers.4.encoder_attention.v_lin.weight', 'decoder.layers.4.encoder_attention.v_lin.bias', 'decoder.layers.4.encoder_attention.out_lin.weight', 'decoder.layers.4.encoder_attention.out_lin.bias', 'decoder.layers.4.norm2.weight', 'decoder.layers.4.norm2.bias', 'decoder.layers.4.ffn.lin1.weight', 'decoder.layers.4.ffn.lin1.bias', 'decoder.layers.4.ffn.lin2.weight', 'decoder.layers.4.ffn.lin2.bias', 'decoder.layers.4.norm3.weight', 'decoder.layers.4.norm3.bias', 'decoder.layers.5.self_attention.q_lin.weight', 'decoder.layers.5.self_attention.q_lin.bias', 'decoder.layers.5.self_attention.k_lin.weight', 'decoder.layers.5.self_attention.k_lin.bias', 'decoder.layers.5.self_attention.v_lin.weight', 'decoder.layers.5.self_attention.v_lin.bias', 'decoder.layers.5.self_attention.out_lin.weight', 'decoder.layers.5.self_attention.out_lin.bias', 'decoder.layers.5.norm1.weight', 'decoder.layers.5.norm1.bias', 'decoder.layers.5.encoder_attention.q_lin.weight', 'decoder.layers.5.encoder_attention.q_lin.bias', 'decoder.layers.5.encoder_attention.k_lin.weight', 'decoder.layers.5.encoder_attention.k_lin.bias', 'decoder.layers.5.encoder_attention.v_lin.weight', 'decoder.layers.5.encoder_attention.v_lin.bias', 'decoder.layers.5.encoder_attention.out_lin.weight', 'decoder.layers.5.encoder_attention.out_lin.bias', 'decoder.layers.5.norm2.weight', 'decoder.layers.5.norm2.bias', 'decoder.layers.5.ffn.lin1.weight', 'decoder.layers.5.ffn.lin1.bias', 'decoder.layers.5.ffn.lin2.weight', 'decoder.layers.5.ffn.lin2.bias', 'decoder.layers.5.norm3.weight', 'decoder.layers.5.norm3.bias', 'decoder.layers.6.self_attention.q_lin.weight', 'decoder.layers.6.self_attention.q_lin.bias', 'decoder.layers.6.self_attention.k_lin.weight', 'decoder.layers.6.self_attention.k_lin.bias', 'decoder.layers.6.self_attention.v_lin.weight', 'decoder.layers.6.self_attention.v_lin.bias', 'decoder.layers.6.self_attention.out_lin.weight', 'decoder.layers.6.self_attention.out_lin.bias', 'decoder.layers.6.norm1.weight', 'decoder.layers.6.norm1.bias', 'decoder.layers.6.encoder_attention.q_lin.weight', 'decoder.layers.6.encoder_attention.q_lin.bias', 'decoder.layers.6.encoder_attention.k_lin.weight', 'decoder.layers.6.encoder_attention.k_lin.bias', 'decoder.layers.6.encoder_attention.v_lin.weight', 'decoder.layers.6.encoder_attention.v_lin.bias', 'decoder.layers.6.encoder_attention.out_lin.weight', 'decoder.layers.6.encoder_attention.out_lin.bias', 'decoder.layers.6.norm2.weight', 'decoder.layers.6.norm2.bias', 'decoder.layers.6.ffn.lin1.weight', 'decoder.layers.6.ffn.lin1.bias', 'decoder.layers.6.ffn.lin2.weight', 'decoder.layers.6.ffn.lin2.bias', 'decoder.layers.6.norm3.weight', 'decoder.layers.6.norm3.bias', 'decoder.layers.7.self_attention.q_lin.weight', 'decoder.layers.7.self_attention.q_lin.bias', 'decoder.layers.7.self_attention.k_lin.weight', 'decoder.layers.7.self_attention.k_lin.bias', 'decoder.layers.7.self_attention.v_lin.weight', 'decoder.layers.7.self_attention.v_lin.bias', 'decoder.layers.7.self_attention.out_lin.weight', 'decoder.layers.7.self_attention.out_lin.bias', 'decoder.layers.7.norm1.weight', 'decoder.layers.7.norm1.bias', 'decoder.layers.7.encoder_attention.q_lin.weight', 'decoder.layers.7.encoder_attention.q_lin.bias', 'decoder.layers.7.encoder_attention.k_lin.weight', 'decoder.layers.7.encoder_attention.k_lin.bias', 'decoder.layers.7.encoder_attention.v_lin.weight', 'decoder.layers.7.encoder_attention.v_lin.bias', 'decoder.layers.7.encoder_attention.out_lin.weight', 'decoder.layers.7.encoder_attention.out_lin.bias', 'decoder.layers.7.norm2.weight', 'decoder.layers.7.norm2.bias', 'decoder.layers.7.ffn.lin1.weight', 'decoder.layers.7.ffn.lin1.bias', 'decoder.layers.7.ffn.lin2.weight', 'decoder.layers.7.ffn.lin2.bias', 'decoder.layers.7.norm3.weight', 'decoder.layers.7.norm3.bias'])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('emelymodels': conda)"
  },
  "interpreter": {
   "hash": "f8da2b72f9ac3c32718e899b9529d2abac5ccc10881893810be77cced976bf1a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}